# image-captioning-and-translation
This project combines image captioning using the BLIP model and English â†’ Arabic translation using a fine-tuned mBART-50 model.
The system runs through a simple Gradio web interface, allowing you to upload an image and receive the caption in English or Arabic.

ğŸš€ Features

ğŸ“· Upload an image and automatically generate a caption

ğŸ§  Uses a fine-tuned BLIP model for English captions

ğŸŒ Uses a fine-tuned mBART model for English â†’ Arabic translation

ğŸ–¥ï¸ Clean Gradio UI

ğŸ—‚ï¸ Organized project structure

ğŸ”§ Easy to run locally or on cloud platforms


ğŸ“‚ Project Directory Structure
.
â”œâ”€â”€ app.py                        # Main Gradio application
â”‚
â”œâ”€â”€ mbart_en_ar_model/            # Fine-tuned mBART model
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ blip/                         # Fine-tuned BLIP model
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ processor_config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ samples/                      # Optional sample images
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

ğŸ§  Models Used

ğŸ”¹ 1. BLIP â€” Image Captioning

Pretrained model: Salesforce/blip-image-captioning-base

Generates high-quality English captions from raw images.

ğŸ”¹ 2. mBART-50 â€” English â†’ Arabic Translation

Base model: facebook/mbart-large-50-many-to-many-mmt

You fine-tuned it on your parallel ENâ†’AR dataset.

Loaded locally from:

mbart_en_ar_model/

ğŸ›  Installation
1ï¸âƒ£ Clone the repository
git clone <your-repo-url>
cd <your-project-folder>

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt


If requirements.txt is missing, use this one:

torch
transformers
gradio


For GPU installation:

pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121

ğŸš€ Running the Application

Run Gradio:

python app.py


Then open the URL that appears:

http://127.0.0.1:7860

ğŸ–¼ Application Workflow

1ï¸âƒ£ Upload Image

User uploads an image via Gradio.

2ï¸âƒ£ BLIP Generates English Caption
inputs = blip_processor(images=image, return_tensors="pt").to(device)
output_ids = blip_model.generate(**inputs)
english_caption = blip_processor.decode(output_ids[0], skip_special_tokens=True)

3ï¸âƒ£ mBART Translates to Arabic (optional)
tokens = mbart_tokenizer(english_caption, return_tensors="pt").to(device)
arabic_ids = mbart_model.generate(**tokens)
arabic_caption = mbart_tokenizer.decode(arabic_ids[0], skip_special_tokens=True)

ğŸ® Gradio Interface (UI)

You can switch between:

English caption

Arabic translation

The UI allows real-time captioning and translation.

ğŸ§ª Example Output
Input Image

English Caption

"A brown dog running across a grassy field."

Arabic Translation

"ÙƒÙ„Ø¨ Ø¨Ù†ÙŠ ÙŠØ¬Ø±ÙŠ Ø¹Ø¨Ø± Ø­Ù‚Ù„ Ø¹Ø´Ø¨ÙŠ."

ğŸ§µ Training Summary for mBART-50

You trained the model using a dataset containing:

en â†’ English sentence

ar â†’ Arabic translation

Core preprocessing logic:

labels = input_ids.clone()
labels[labels == tokenizer.pad_token_id] = -100


Why -100?
Because PyTorchâ€™s CrossEntropyLoss ignores -100, so padding tokens donâ€™t affect training.

ğŸ›£ Future Improvements

 Add Arabic â†’ English translation

 Fine-tune BLIP for Arabic

 Add attention heatmap visualization

 Deploy app to HuggingFace Spaces

 Add multiple language options

ğŸ“œ License

This project is licensed under the MIT License.

ğŸ™Œ Acknowledgements

Salesforce Research for BLIP

Meta AI for mBART-50

HuggingFace for model hub + transformers

Gradio for UI framework
