Image Captioning & Translation using BLIP + mBART

This project implements an AI-powered image captioning system that generates descriptive captions from images and optionally translates them into Arabic. It uses a hybrid pipeline combining BLIP for image caption generation and mBART-50 for multilingual translation.

The goal of the project is to explore visionâ€“language models and understand how image understanding can be integrated with neural machine translation to produce coherent captions in multiple languages.

Project Overview

Built an image-to-text captioning system using BLIP (Bootstrapping Language-Image Pre-training)

Integrated mBART-50 for English â†’ Arabic translation

Trained and fine-tuned mBART on a bilingual Englishâ€“Arabic dataset

Designed a simple Gradio-based web interface

Supports caption generation in English or Arabic

Enables real-time image upload and caption prediction

Key Concepts

Computer Vision

Natural Language Processing (NLP)

Image Captioning

Transformer Models

Visionâ€“Language Models

Neural Machine Translation (NMT)

Text Tokenization and Sequence Generation

Model Fine-Tuning

Model Architecture
1. BLIP â€” Image Encoder & Caption Generator

Extracts visual features from the input image

Generates an initial English description

Provides high-quality captions with visionâ€“text attention

2. mBART-50 â€” Machine Translation

Receives the English caption from BLIP

Translates it into Arabic using a fine-tuned many-to-many model

Handles tokenization, sequence generation, and decoding

3. Combined Pipeline
Image â†’ BLIP â†’ English Caption â†’ mBART (optional) â†’ Arabic Caption

Dataset (for Translation Fine-Tuning)

The mBART model was fine-tuned on a parallel Englishâ€“Arabic dataset.

Preprocessing steps included:

Lowercasing

Removing special characters

Text normalization

Tokenization

Padding and truncation

Building DataLoader for batches

Results

Generates accurate and coherent English captions from images

Produces high-quality Arabic translations of the generated captions

Demonstrates the strength of combining visionâ€“language and NMT models

Fully deployable through a Gradio GUI

GUI Access

A simple and interactive GUI was built using Gradio:

Upload an image

Choose the output language (English or Arabic)

Receive a generated caption instantly

Project Directory Structure
ðŸ“‚ Project Directory Structure
.
â”œâ”€â”€ app.py                      # Main Gradio application
â”‚
â”œâ”€â”€ mbart_en_ar_model/          # Fine-tuned mBART model
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ blip/                       # Fine-tuned BLIP model (optional if saved locally)
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ processor_config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ samples/                    # Sample input images
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

Team Members

Nada Maher

Mohamed Elhady

Future Improvements

Add support for more output languages

Improve translation accuracy with larger bilingual datasets

Fine-tune BLIP for custom domain captioning

Add temperature, top-k, and top-p sampling controls

Deploy on HuggingFace Spaces or a custom web server

Add support for GPU-accelerated inference in production

License

This project is created for educational and research purposes.
